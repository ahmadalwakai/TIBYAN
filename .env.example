# ===========================================
# Tibyan LMS - Environment Variables
# ===========================================
# Copy this file to .env and fill in your values

# Database (Required)
# PostgreSQL connection string - Use Vercel Postgres, Neon, Supabase, or any PostgreSQL provider
DATABASE_URL="postgresql://user:password@host:5432/tibyan?sslmode=require"

# Direct database URL (for Prisma migrations in serverless)
DIRECT_DATABASE_URL="postgresql://user:password@host:5432/tibyan?sslmode=require"

# Authentication (Required for production)
# Generate with: openssl rand -base64 32
NEXTAUTH_SECRET="your-nextauth-secret-key-here"
NEXTAUTH_URL="https://your-domain.vercel.app"

# Application
NEXT_PUBLIC_APP_URL="https://your-domain.vercel.app"
NEXT_PUBLIC_APP_NAME="تبيان"

# Optional: Analytics & Monitoring
# SENTRY_DSN="https://xxx@sentry.io/xxx"
# NEXT_PUBLIC_GOOGLE_ANALYTICS="G-XXXXXXXXXX"

# Optional: Email Service (for notifications)
# SMTP_HOST="smtp.example.com"
# SMTP_PORT="587"
# SMTP_USER="user@example.com"
# SMTP_PASSWORD="your-smtp-password"
# SMTP_FROM="noreply@tibyan.com"

# Email Service with Resend (Required for auth emails)
# Get your API key from https://resend.com
RESEND_API_KEY="re_xxxxxxxxxxxxxxxxxxxxxxxxxxxx"
FROM_EMAIL="noreply@tibyan.com"

# Optional: Payment Gateway
# STRIPE_SECRET_KEY="sk_live_xxx"
# STRIPE_WEBHOOK_SECRET="whsec_xxx"
# NEXT_PUBLIC_STRIPE_PUBLISHABLE_KEY="pk_live_xxx"

# Optional: File Storage (Vercel Blob, Cloudinary, or S3)
# BLOB_READ_WRITE_TOKEN="vercel_blob_xxx"
# CLOUDINARY_CLOUD_NAME="your-cloud-name"
# CLOUDINARY_API_KEY="xxx"
# CLOUDINARY_API_SECRET="xxx"

# Optional: Redis (for caching/sessions)
# REDIS_URL="redis://default:xxx@xxx.upstash.io:6379"

# Optional: Local AI Agent (llama.cpp server)
# LLM Provider: "local" | "mock" | "auto" (default: auto)
# - auto: tries local first, falls back to mock
# - local: forces local llama-server, fails if unavailable
# - mock: always uses mock provider (for testing/dev, no LLM required)
LLM_PROVIDER="auto"

# llama-server configuration
LLAMA_SERVER_URL="http://127.0.0.1:8080"

# LLM timeouts (in milliseconds)
LLM_TIMEOUT_MS="120000"
LLM_HEALTH_TIMEOUT_MS="1500"

# llama-server model settings
LLM_CONTEXT_SIZE="2048"
LLM_N_GPU_LAYERS="0"

# Auto-start llama-server in development (requires model path)
# AUTO_START_LLM="true"
# LLM_MODEL_PATH="c:\\tibyan\\AI Agent\\qwen2.5-3b-instruct-q4_k_m.gguf"
# LLAMA_SERVER_PATH="c:\\tibyan\\AI Agent\\llama.cpp\\build\\bin\\llama-server.exe"

# Health check retries
LLM_HEALTH_RETRIES="3"
LLM_HEALTH_RETRY_DELAY_MS="1000"

# Debug mode (admin-only, shows provider details in API responses)
# DEBUG_AI="false"
