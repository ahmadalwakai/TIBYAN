# ===========================================
# Tibyan LMS - Environment Variables
# ===========================================
# Copy this file to .env and fill in your values
# See VERCEL_DEPLOYMENT.md for production setup

# ===========================================
# DATABASE (Required)
# ===========================================
# PostgreSQL connection string - Use Vercel Postgres, Neon, Supabase, etc.
DATABASE_URL="postgresql://user:password@host:5432/tibyan?sslmode=require"

# Direct database URL (for Prisma migrations in serverless environments)
DIRECT_DATABASE_URL="postgresql://user:password@host:5432/tibyan?sslmode=require"

# ===========================================
# AUTHENTICATION (Required for production)
# ===========================================
# JWT secret for custom auth (NOT NextAuth - this app uses custom JWT cookies)
# Generate with: openssl rand -hex 32
JWT_SECRET="your-64-character-hex-secret-here"

# ===========================================
# APPLICATION
# ===========================================
NEXT_PUBLIC_APP_URL="https://your-domain.vercel.app"
NEXT_PUBLIC_APP_NAME="تبيان"

# ===========================================
# EMAIL SERVICE (Required for auth emails)
# ===========================================
# Get your API key from https://resend.com
RESEND_API_KEY="re_xxxxxxxxxxxxxxxxxxxxxxxxxxxx"
FROM_EMAIL="noreply@your-domain.com"

# ===========================================
# LLM / AI AGENT CONFIGURATION
# ===========================================
# LLM Provider: "mock" | "local" | "remote" | "auto" (default: auto)
# - auto: tries local first (dev), falls back to mock
# - local: forces local llama-server (dev only, will fail on Vercel)
# - remote: uses external LLM API (OpenAI-compatible endpoint)
# - mock: safe fallback responses (no real LLM)
LLM_PROVIDER="auto"

# --- Local Development (llama-server) ---
# Only used when LLM_PROVIDER="local" or "auto" in dev
LLAMA_SERVER_URL="http://127.0.0.1:8080"
LLM_TIMEOUT_MS="120000"
LLM_HEALTH_TIMEOUT_MS="1500"
LLM_CONTEXT_SIZE="2048"
LLM_N_GPU_LAYERS="0"
LLM_HEALTH_RETRIES="3"
LLM_HEALTH_RETRY_DELAY_MS="1000"

# --- Remote/Production LLM (OpenAI-compatible API) ---
# Required when LLM_PROVIDER="remote" (for Vercel deployment)
# REMOTE_LLM_BASE_URL="https://api.openai.com/v1"
# REMOTE_LLM_API_KEY="sk-xxxxxxxxxxxxxxxxxxxx"
# REMOTE_LLM_MODEL="gpt-4o-mini"

# ===========================================
# OPTIONAL: ANALYTICS & MONITORING
# ===========================================
# SENTRY_DSN="https://xxx@sentry.io/xxx"
# NEXT_PUBLIC_GOOGLE_ANALYTICS="G-XXXXXXXXXX"

# ===========================================
# OPTIONAL: FILE STORAGE
# ===========================================
# BLOB_READ_WRITE_TOKEN="vercel_blob_xxx"
# CLOUDINARY_CLOUD_NAME="your-cloud-name"
# CLOUDINARY_API_KEY="xxx"
# CLOUDINARY_API_SECRET="xxx"

# ===========================================
# OPTIONAL: REDIS (caching/sessions)
# ===========================================
# REDIS_URL="redis://default:xxx@xxx.upstash.io:6379"

# ===========================================
# OPTIONAL: PAYMENTS
# ===========================================
# STRIPE_SECRET_KEY="sk_live_xxx"
# STRIPE_WEBHOOK_SECRET="whsec_xxx"
# NEXT_PUBLIC_STRIPE_PUBLISHABLE_KEY="pk_live_xxx"

# ===========================================
# DEBUG (Development only)
# ===========================================
# DEBUG_AI="true"
